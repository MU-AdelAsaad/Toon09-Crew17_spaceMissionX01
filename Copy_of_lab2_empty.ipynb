{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MU-AdelAsaad/Toon09-Crew17_spaceMissionX01/blob/main/Copy_of_lab2_empty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "od2YkWY8pSOC"
      },
      "source": [
        "# NLP 2024\n",
        "# Lab 2: Word Embeddings\n",
        "\n",
        "Throughout the course we have discussed different ways to represent a word. The latest one (and the most successful in terms of results) is using word embeddings: we represent each word as a vector consisting of numbers. The vector encodes the meaning of the word. These numbers (or weights) for each word are learned using various machine learning models. In this lab, we explore how to create such vectors given a corpus (note that in the real world, you can also load the trained word vectors, and you will almost never have to train them from scratch), which properties they have and what kind of tasks we can solve with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "s93LS5bspSOD"
      },
      "source": [
        "By the end of this lab you should be able to:\n",
        "- Implement and/or use built-in functions to preprocess your data (once again)\n",
        "- Execute `word2vec` based on a corpus\n",
        "- Inspect and test word embeddings properties\n",
        "- Use word embeddings to get sentence representations (aka sentence embeddings)\n",
        "- Use sentence embeddings to solve more complicated tasks like information retrieval\n",
        "- Design evaluation frameworks for specific NLP tasks and assess their difficulty\n",
        "\n",
        "### Score breakdown\n",
        "\n",
        "Exercise | Points\n",
        "--- | ---\n",
        "[Exercise 1](#e1) | 3\n",
        "[Exercise 2](#e2) | 3\n",
        "[Exercise 3](#e3) | 3\n",
        "[Exercise 4](#e4) | 5\n",
        "[Exercise 5](#e5) | 10\n",
        "[Exercise 6](#e6) | 6\n",
        "[Exercise 7](#e7) | 5\n",
        "[Exercise 8](#e8) | 5\n",
        "[Exercise 9](#e9) | 5\n",
        "[Exercise 10](#e10) | 5\n",
        "[Exercise 11](#e11) | 5\n",
        "[Exercise 12](#e12) | 15\n",
        "Total | 70\n",
        "\n",
        "This score will be scaled down to 1 and that will be your final lab score.\n",
        "\n",
        "### Instructions for delivery (Deadline: 17/May late night, wildcards possible)\n",
        "\n",
        "+ Make sure that you include a proper amount/mix of comments, results and code.\n",
        "+ In the end, make sure that all cells are executed properly and everything you need to show is in your (execucted) notebook.\n",
        "+ You are asked to deliver only your executed notebook file, .ipnyb and nothing else. Enjoy!\n",
        "+ Honor code applies to these tasks. Only individual work should be submitted.\n",
        "+ While you may talk with others about this lab, we ask that you write your solutions individually. If you do discuss specific tasks with others please include their names below.\n",
        "+ It is mandatory to list and disclose any website (or other resource) you used (e.g. stackoverflow) as well as any genAI tools (e.g. chatGPT) used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhGHJT8JN_y0"
      },
      "source": [
        "Collaborators: list collaborators here\n",
        "\n",
        "**I talked with Jerry about...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FuY0XXAOF7c"
      },
      "source": [
        "Use of genAI tools (e.g. chatGPT), websites (e.g. stackoverflow): list websites where you found code (or other info) as well as include information on how you used genAI tools (e.g. prompts):\n",
        "\n",
        "I asked chatGPT about..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aHYGq5RUpSOD"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "As in the last lab, we will be using huggingface datasets library ([https://huggingface.co/datasets](https://huggingface.co/datasets)). You can find the detailed documentation and tutorials here: [https://huggingface.co/docs/datasets/en/index](https://huggingface.co/docs/datasets/en/index)\n",
        "\n",
        "If you don't have it installed you can run the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhEG8hKrpSOE"
      },
      "outputs": [],
      "source": [
        "! pip install -U datasets~=2.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5CCI1TPJpSOF"
      },
      "source": [
        "As usual, we start by importing some essential Python libraries and we will be using. Apart from `gensim` (which is going to be used for word2vec), we have already seen the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRe8W4hKpSOF"
      },
      "outputs": [],
      "source": [
        "import re  # for regular expressions\n",
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datasets\n",
        "import gensim\n",
        "import nltk\n",
        "import tqdm\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "NXy-1KiCpSOF"
      },
      "source": [
        "## 1. Load and Preprocess Data\n",
        "\n",
        "*Sentence compression* involves rephrasing sentences to make them shorter while still retaining the original meaning. A reliable compression system would be valuable for mobile devices and could also serve as a component in an extractive summarization system.\n",
        "\n",
        "The dataset we are going to use can be found on [Huggingface](https://huggingface.co/datasets/embedding-data/sentence-compression). It concerns a set of 180,000 pairs of sentences, aka it is a parallel corpus of sentences and their equivalent compressions. It has been collected by harvesting news articles from the Internet where the headline appears to be similar to the first sentence and that property is used to find an \"extractive\" compression of the sentence.\n",
        "\n",
        "For example, for the sentence\n",
        "\n",
        "`\"Regulators Friday shut down a small Florida bank, bringing to 119 the number of US bank failures this year amid mounting loan defaults\"`\n",
        "\n",
        "the compressed equivalent (based on the dataset) is:\n",
        "\n",
        "`\"Regulators shut down small Florida bank\"`.\n",
        "\n",
        "\n",
        "For more information you can read the original paper (from Google) [here](https://aclanthology.org/D13-1155.pdf). We strongly recommend going over the paper to gain further insights. Notice that the paper is from 2013, therefore word embeddings have not been widely introduced yet in NLP tasks, meaning that the methods applied were based on the traditional NLP pipeline (feature extraction + ML)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ztJX2GRVpSOF"
      },
      "source": [
        "### 1.1 Loading the Dataset\n",
        "\n",
        "The dataset will be loaded as a Pandas DataFrame. This may take a few minutes because of the large size of the data.\n",
        "\n",
        "Make sure to inspect the dataset and make sure it is imported properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tay2NXPTpSOG"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset('embedding-data/sentence-compression')\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUud3eZVpSOH"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(ds['train'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "HfJSiWxrpSOH"
      },
      "source": [
        "The dataset comes with only the `train` split so we will have to split it ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tATL53MWpSOH"
      },
      "outputs": [],
      "source": [
        "split_ds = ds['train'].train_test_split(test_size=0.2)\n",
        "print(split_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "L133t3CqpSOH"
      },
      "source": [
        "### 1.2 Preprocessing the dataset\n",
        "In this section we will prepare the dataset, aka clean the sentences and tokenizem.\n",
        "\n",
        "First, let's write the function to clean the text. It can be similar to the one from the previous lab (Lab1) but make sure that it makes sense for this dataset and task.\n",
        "\n",
        "More specifically, think about lower-casing, punctuation, stop-words and lemmatization/stemming and the impact it might have on the dataset. Also reflect on the fact that with word2vec we want to uncover semantic relationships between words, whereas with bag-of-words we were trying to capture different morphological variations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mQszN6GNpSOH"
      },
      "source": [
        "<a name='e1'></a>\n",
        "### Exercise 1 (points 3)\n",
        "Fill in the following function ot clean the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxjfLQj6pSOH"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    \"\"\"\n",
        "    Cleans the given text\n",
        "    Args:\n",
        "        text: a str with the text to clean\n",
        "\n",
        "    Returns: a str with the cleaned text\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Empty text\n",
        "    if type(text) not in (str, np.str_) or text == '':\n",
        "        print('empty text', type(text), type(text) not in (str, np.str_), text)\n",
        "        return ''\n",
        "\n",
        "    # 'text' from the example can be of type numpy.str_, let's convert it to a python str\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(\"\\'s\", \" \",\n",
        "                  text)  # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
        "    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(\"\\'ve\", \" have \", text)\n",
        "\n",
        "    #you might need more\n",
        "    #add them here\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "\n",
        "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
        "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    # Update the example with the cleaned text\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gAqf1AoupSOJ"
      },
      "source": [
        "The following function will apply the function (sic) you just wrote to the whole dataset. More specifically, it takes the first entry (`sentence`) from the set of uncompressed/compressed pairs, applies the `clean` function and saves the processed sentence in the field `clean_sentence`. The same is dome for the compressed version of the sentence (saved as `clean_compressed`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZn-M4_vpSOJ"
      },
      "outputs": [],
      "source": [
        "def clean_dataset(example):\n",
        "    \"\"\"\n",
        "    Cleans the sentence and compressed sentence in the example from the Dataset\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: updated example with 'clean_sentence' and 'clean_compressed' cleaned\n",
        "\n",
        "    \"\"\"\n",
        "    sentence, compressed = example['set']\n",
        "    clean_sentence = clean(sentence)\n",
        "    clean_compressed = clean(compressed)\n",
        "    example['clean_sentence'] = clean_sentence\n",
        "    example['clean_compressed'] = clean_compressed\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_KVujosKpSOJ"
      },
      "source": [
        "Below we apply the function to the whole dataset (using `map`) and we can also inspect the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBS1JbISpSOJ"
      },
      "outputs": [],
      "source": [
        "split_ds = split_ds.map(clean_dataset)\n",
        "print(split_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "X_ThE9H-pSOJ"
      },
      "source": [
        "Let's examine some examples from the dataset and make sure that we got the results we wanted. At this step, it might be necessary to revisit some pre-processing steps if you are not happy with the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP-qOPijpSOJ"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(split_ds['train'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "7LI2KlfypSOJ"
      },
      "source": [
        "<a name='e2'></a>\n",
        "### Exercise 2 (points 3)\n",
        "\n",
        "As always, we will need to tokenize the dataset in order to train our own `word2Vec` model in the next section. We will use the [Natural Language Toolkit (NLTK) library]([https://www.nltk.org/]) (https://www.nltk.org/).\n",
        "\n",
        "Complete the following function to split the text into tokens using the `word_tokenize()` function. Check the [documentation](https://www.nltk.org/api/nltk.tokenize.word_tokenize.html?highlight=word_tokenize) first.\n",
        "\n",
        "Note that there are different tokenizers e.g. `RegexpTokenizer` where you can enter your own regexp, `WhitespaceTokenizer` (similar to Python's string.split()) and `BlanklineTokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb-M3V6ApSOK"
      },
      "outputs": [],
      "source": [
        "# if you want you can remove the stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the `text` parameter using nltk library\n",
        "    Args:\n",
        "        text: a string representing a sentence to be tokenized\n",
        "\n",
        "    Returns: a list of tokens (strings)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gwsHqZc9pSOK"
      },
      "source": [
        "Next, the function will be applied to the whole dataset (as we did with the pre-processing) and `sentence_tokens` field will be created to store the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sTX-AcTpSOK"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(example):\n",
        "    \"\"\"\n",
        "    Tokenizes 'clean_sentence' columns in the example from the Dataset\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: updated example with 'sentence_tokens' columns\n",
        "\n",
        "    \"\"\"\n",
        "    cleaned_sentence = example['clean_sentence']\n",
        "    tokens = tokenize(cleaned_sentence)\n",
        "    example['sentence_tokens'] = tokens\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvwmXLj2pSOK"
      },
      "outputs": [],
      "source": [
        "split_ds = split_ds.map(tokenize_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXnje31SpSOK"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(split_ds['train'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FeF0wFd0pSOL"
      },
      "source": [
        "Since we will need the tokenized sentences, we can use the following statement to extract them from the `train` split of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYKfXFxQpSOL"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences = split_ds['train']['sentence_tokens']\n",
        "print(len(tokenized_sentences))\n",
        "print(tokenized_sentences[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDGwEiZeCA5U"
      },
      "source": [
        "Notice the difference in the types of the different structures we use. Run the following cell to check the types. Do they make sense to you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKrkFmcZCtOd"
      },
      "outputs": [],
      "source": [
        "#type of original dataset\n",
        "print(type(split_ds))\n",
        "print(\"--\")\n",
        "#type of original sentence\n",
        "print(split_ds['train'][1])\n",
        "print(type(split_ds['train'][1]))\n",
        "print(\"--\")\n",
        "#type of pre-proceesed sentence\n",
        "print(split_ds['train']['clean_sentence'][1])\n",
        "print(type(split_ds['train']['clean_sentence'][1]))\n",
        "print(\"--\")\n",
        "#type of tokenized sentence\n",
        "print(split_ds['train']['sentence_tokens'][1])\n",
        "print(type(split_ds['train']['sentence_tokens'][1]))\n",
        "print(\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "o3goFRrIpSOL"
      },
      "source": [
        "## 2. Word2Vec word embeddings\n",
        "\n",
        "Before proceeding below make sure that you understand the way Word2Vec words. Refer to the lecture slides and/or the textbooks. Here we provide a (very) rough introduction that is lacking many details we discussed in class.\n",
        "\n",
        "Word2vec operates on the principle that words appearing in similar contexts tend to have similar meanings. There are two main algoriths with diffferent variations. in Skip-gram the model precits the context words given the target word, whereas in CBOW the model predicts the target word given its context words (words surrounding it). During training (which can take a lot of time in case of big corpora), word2Vec updates the vector representations of words to maximize the probability of correctly predicting the target or context words. If we use a neural network, training is done with gradient descent.\n",
        "\n",
        "We need to decide on different parameters that will influence the behavior and performance of word2Vec models. These include the dimensionality of the word vectors which determines the size of the vector space in which words are embedded, and the window (context) size, which defines the number of context words considered in each training instance. Additionally, the choice of training algorithm (CBOW or Skip-gram), learning rate, number of training epochs need to be tuned.\n",
        "\n",
        "Nough said, it's time to train our own Word2Vec embeddings. We will use the powerful [Gensim library](https://radimrehurek.com/gensim/models/word2vec.html). Make sure to read the documentation to decrypt the parameters we need to tune (embedding vector size, context window, type of algorithm etc.). The training might take some time, depending on the parameters you use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JgNOhCGpSOL"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "embedding_size = 100\n",
        "w2v_model = Word2Vec(tokenized_sentences, vector_size=embedding_size, min_count=5, window=5, sg=1, hs=0, seed=1,\n",
        "                     workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEUne07ppSOL"
      },
      "outputs": [],
      "source": [
        "w2v_model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "g57Y44oDpSOL"
      },
      "source": [
        "The Word2Vec model has `wv` attribute. We can use it to retrieve the whole vocabulary (aka for how many words we learned embeddings for)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GteyeTL1pSOL"
      },
      "outputs": [],
      "source": [
        "vocab = list(w2v_model.wv.key_to_index)\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "G7qvmk1SpSOL"
      },
      "source": [
        "Here you can see the word cloud created from the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Z2C-ROpSOM"
      },
      "outputs": [],
      "source": [
        "# visualizing our vocab with a word cloud.\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Generate a word cloud image for words\n",
        "combined_vocab = ' '.join(vocab)\n",
        "wordcloud = WordCloud(width=600, height=400).generate(combined_vocab)\n",
        "plt.figure(figsize=(9, 9))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9ZuIY05bpSOM"
      },
      "source": [
        "Let's explore a bit further the embeddings. In the following cells, the embedding of a single word is returned. Double-check the dimensions (as sanity check). This is like inspecting the `W` matrix (weights) that we discussed in the lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l63gJVdspSOM"
      },
      "outputs": [],
      "source": [
        "# vector of a particular model. note that it is 100 dimensional as specified.\n",
        "w2v_model.wv['what']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e9p9a_NpSOM"
      },
      "outputs": [],
      "source": [
        "# can also do like this.\n",
        "w2v_model.wv['apple']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "maFDaNnVpSOM"
      },
      "source": [
        "`word2vec` offers different functions to easily run very common tasks. For example, there are different functions to find the most similar words.\n",
        "\n",
        "Check the documentation on how [`most_similar`](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html) and [`similar_by_word`](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.similar_by_word.html) can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdvIgRYjpSOM"
      },
      "outputs": [],
      "source": [
        "# most similar words to a given word\n",
        "print(w2v_model.wv.most_similar('what', topn=10))\n",
        "\n",
        "# also u can use\n",
        "print(w2v_model.wv.similar_by_word('miss', topn=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0akFfTadpSOM"
      },
      "outputs": [],
      "source": [
        "print(w2v_model.wv.most_similar('why', topn=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CJJ77ImpSOM"
      },
      "outputs": [],
      "source": [
        "print(w2v_model.wv.similar_by_word('who', topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4eKpAp1ppSON"
      },
      "source": [
        "<a name='e3'></a>\n",
        "### Exercise 3 (points 3)\n",
        "\n",
        "While there is already a similarity metric in the `word2Vec` class, we will write our own implementation of the cosine similarity. Complete the following funciton that given any two vectors will compute the cosine similarity. If you don't remember the formula for the cosine similarity, revisit the course material. Notice that the function receives numpy arrays and recall that you can express cosine similarity as a dot product. Use numpy functions to write an efficient implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6WLJAhppSON"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity between two vectors\n",
        "    Args:\n",
        "        vector1: numpy array of the first vector\n",
        "        vector2: numpy array of the second vector\n",
        "\n",
        "    Returns: cosine similarity\n",
        "\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPoY-9jdpSON"
      },
      "outputs": [],
      "source": [
        "cosine_similarity(np.array([0, 1, 2]), np.array([0, 1, 2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ta6EVhP8pSON"
      },
      "source": [
        "We can now compare our implementation with the one in the Word2Vec model and confirm what we already expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90_oKIp4pSON"
      },
      "outputs": [],
      "source": [
        "# simalarity between two words\n",
        "word1 = 'alive'\n",
        "word2 = 'biology'\n",
        "print(w2v_model.wv.similarity(word1, word2))\n",
        "print(cosine_similarity(w2v_model.wv[word1], w2v_model.wv[word2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1MaE9SdpSON"
      },
      "outputs": [],
      "source": [
        "# simalarity between two words. similar words\n",
        "word1 = 'alive'\n",
        "word2 = 'life'\n",
        "print(w2v_model.wv.similarity(word1, word2))\n",
        "print(cosine_similarity(w2v_model.wv[word1], w2v_model.wv[word2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYstyUqgpSOO"
      },
      "outputs": [],
      "source": [
        "# simalarity between two words. dissimilar words\n",
        "word1 = 'alive'\n",
        "word2 = 'dead'\n",
        "print(w2v_model.wv.similarity(word1, word2))\n",
        "print(cosine_similarity(w2v_model.wv[word1], w2v_model.wv[word2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coXQeTHtpSOO"
      },
      "outputs": [],
      "source": [
        "# simalarity between two words. unrelated words\n",
        "word1 = 'alive'\n",
        "word2 = 'horse'\n",
        "print(w2v_model.wv.similarity(word1, word2))\n",
        "print(cosine_similarity(w2v_model.wv[word1], w2v_model.wv[word2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp8pQi_tpSOO"
      },
      "outputs": [],
      "source": [
        "# simalarity between two SAME words\n",
        "w2v_model.wv.similarity('equal', 'equal')\n",
        "word1 = 'equal'\n",
        "word2 = 'equal'\n",
        "print(w2v_model.wv.similarity(word1, word2))\n",
        "print(cosine_similarity(w2v_model.wv[word1], w2v_model.wv[word2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "8pNrM_OzpSOO"
      },
      "source": [
        "The next function contains the code to plot a similarity matrix between multiple words (e.g. if we want to compare 10 words and their pair-wise similarities). It requires a matrix with similarities (as input) and labels (aka the words) to display in the final figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so8fd0SxpSOO"
      },
      "outputs": [],
      "source": [
        "def plot_similarity_matrix(matrix, labels):\n",
        "    \"\"\"\n",
        "    Displays a plot of the `matrix` of size (N x N) with the labels specified as a list of size N\n",
        "    Args:\n",
        "        matrix: a square-sized (N x N) numpy array\n",
        "        labels: a list of strings of hte size N\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(matrix)\n",
        "\n",
        "    # Show all ticks and label them with the respective list entries\n",
        "    ax.set_xticks(np.arange(len(labels)), labels=labels)\n",
        "    ax.set_yticks(np.arange(len(labels)), labels=labels)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(len(labels)):\n",
        "        for j in range(len(labels)):\n",
        "            text = ax.text(j, i, f'{matrix[i, j]:.2f}',\n",
        "                           ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "    # ax.set_title(\"Give a title if you want\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Yc-n_vLqpSOP"
      },
      "source": [
        "<a name='e4'></a>\n",
        "### Exercise 4 (points 5)\n",
        "\n",
        "In the following, we will explore some properties of word embeddings through some examples. We will use 6 example words for this purpose but feel free to explore some of your own (also for Exercise 5).\n",
        "\n",
        "First, fill in the next cell to create a similarity matrix between a list of words. Then, call the function given above that plots the similarity matrix, along with the word labels.\n",
        "\n",
        "In the cell, below comment on the results. Based on the word2vec parameters you explored, do they make sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgXy4jJqpSOP"
      },
      "outputs": [],
      "source": [
        "list_of_words = ['love', 'hate', 'life', 'equal', 'alive', 'dead']\n",
        "\n",
        "similarity_matrix = np.zeros((len(list_of_words), len(list_of_words)), dtype=float)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE\n",
        "\n",
        "\n",
        "plot_similarity_matrix(similarity_matrix, list_of_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2hkWYXUppSOP"
      },
      "source": [
        "<a name='e5'></a>\n",
        "### Exercise 5 (points 10)\n",
        "\n",
        "For this exercise, experiment with different words and their similarities plotted. Also, take into account the different parameters of word2vec and more specifically the window size (context) and the role it plays in the types of embeddings we can learn.\n",
        "\n",
        "What happens if I pick a small context window?\n",
        "What happens if I pick a large context window?\n",
        "\n",
        "Are there noticeable patterns? Are they expected? Try to find examples that word embeddings make sense (based on your hypothesis) and not.\n",
        "\n",
        "Comment on your findings below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54jUlpBtpSOP"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGgIcj-XKhQj"
      },
      "source": [
        "// your comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8jd74kOaNDs"
      },
      "source": [
        "## 3. Pre-trained embeddings\n",
        "\n",
        "In this short section you will load the pre-trained embeddings of your choice. Gensim library maintains a storage containing some pre-trained models. You can read more about it [here](https://github.com/piskvorky/gensim-data) ([https://github.com/piskvorky/gensim-data](https://github.com/piskvorky/gensim-data)). Be sure to read the README of this repository.\n",
        "\n",
        "Let's first load the info of what models are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbIetJQwaO5Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import gensim.downloader as api\n",
        "\n",
        "info = api.info()  # show info about available models/datasets\n",
        "print(json.dumps(info['models'], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoHdEhRjaQj5"
      },
      "source": [
        "<a name='e6'></a>\n",
        "### Exercise 6 (points 6)\n",
        "\n",
        "Load the pre-trained word embeddings model of your choice (like we did in the lecture: e.g. try different pre-trained corpora and different types of embeddings (like GloVe or FastText)). You can use the gensim implementation and functionality to download and load several types of embeddings, but you are not limited to it.\n",
        "\n",
        "Next, perform the analysis of the similarities between words as you did in Exercise 5. Compare your trained word2vec with the pretrained model you downloaded. Try to explain the numerical differences based on what you know on how these models were trained.\n",
        "\n",
        "Note: Pre-trained embeddings might be difficult to load and use (memory requirements) so you might have to work locally there. You don't need to submit (really, don't!) the pre-trained word embeddings you used but make sure to include a link in your delivered notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE4QBipIaTf5"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0Vu2FT0aYqm"
      },
      "source": [
        "// your comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNYgm-_8pFE"
      },
      "source": [
        "If you encounter problems with not enough RAM you can modify and run the following cell to unload the pretrained model from memory. Ofcourse, change the variable name to whatever you used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnlsUvq7vMt4"
      },
      "outputs": [],
      "source": [
        "# del fasttext_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Sps-XKnYpSOP"
      },
      "source": [
        "## 4. Sentence Embeddings by Averaging Word2Vec Embeddings\n",
        "\n",
        "Word embeddings are a powerful model for representing words and their meaning (in terms of distributional similarity). As we discussed in class, we can use them in a wide variety of tasks with more complex architectures. Word vectors offer a dense vector for each word. What if we wanted to represent a sentence (or a document) based on word vectors. How can we do that?\n",
        "\n",
        "In the course, we will see different architectures that take into account the sequence of words (by combining their vectors). A first naive but simple and sometimes (as we are going to see) quite effective approach would be to represent a sentence with an embedding vector that is the average of the word vectors that form the sentence.\n",
        "\n",
        "So formally, this is what we are aiming for:\n",
        "\n",
        "$\n",
        "\\text{Sentence_Embedding} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Word_Embedding}_i\n",
        "$\n",
        "\n",
        "where:\n",
        "* $N$ is the number of words in a sentence\n",
        "* $\\text{Word_Embedding}_i$ is the word vector for the $i$-th in the sentence.\n",
        "\n",
        "Things to note:\n",
        "* The embedding vector for the sentence will obviously have the same dimension as the word embedding.\n",
        "* This representation ignores the word order (like bag-of-words). During the course we will see how we can overcome this limitation by using sequence models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xLfwJ56qpSOP"
      },
      "source": [
        "<a name='e7'></a>\n",
        "### Exercise 7 (points 5)\n",
        "\n",
        "Complete the function below that takes as input the sentence in the form of tokens (so it's a list of words) and calculates the sentence embedding vector. First, we would need to retrieve the word embeddings for each word from our trained Word2Vec model and then average the vectors.\n",
        "\n",
        "Note: There can be cases where all tokens from a sentence are out-of-vocabulary words (OOV). Return None in that case. We will filter out the sentences where it occurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1wvAMGWpSOP"
      },
      "outputs": [],
      "source": [
        "def embed_sentence_word2vec(tokens):\n",
        "    \"\"\"\n",
        "    Calculates the sentence embedding by averaging the embeddings of the tokens\n",
        "    Args:\n",
        "        tokens: a list of words from the sentence\n",
        "\n",
        "    Returns: a numpy array of the sentence embedding\n",
        "\n",
        "    \"\"\"\n",
        "    #### YOUR CODE HERE\n",
        "    #### CAUTION: be sure to cover the case where all tokens are out-of-vocabulary!!!\n",
        "    #### Return None in that case - we will filter out those cases later.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qy7_6CLWpSOP"
      },
      "source": [
        "Now we can apply the function to the whole dataset. Here we do it both for the sentence and the compressed version. You should know it by now, but this operation might take some time. The next cells will apply your function to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6NV2jaspSOQ"
      },
      "outputs": [],
      "source": [
        "def embed_sentence_word2vec_dataset(example):\n",
        "    \"\"\"\n",
        "    Embeds the sentence and the compressed sentence in the example from the Dataset\n",
        "    Args:\n",
        "        example: an example from the Dataset\n",
        "\n",
        "    Returns: updated example with 'sentence_word2vec' and 'compressed_word2vec' columns\n",
        "\n",
        "    \"\"\"\n",
        "    sentence_tokens = example['sentence_tokens']\n",
        "    clean_compressed = example['clean_compressed']\n",
        "    compressed_tokens = tokenize(clean_compressed)\n",
        "\n",
        "    sentence_embedding = embed_sentence_word2vec(sentence_tokens)\n",
        "    compressed_embedding = embed_sentence_word2vec(compressed_tokens)\n",
        "\n",
        "    example['sentence_word2vec'] = sentence_embedding\n",
        "    example['compressed_word2vec'] = compressed_embedding\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_HA6hB4pSOQ"
      },
      "outputs": [],
      "source": [
        "split_ds = split_ds.map(embed_sentence_word2vec_dataset)\n",
        "print(split_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8scTXZy8b45s"
      },
      "source": [
        "Here we will filter out the examples where sentences (or their compressed versions) are composed of only OOV words. Notice the changed size of the dataset after the operation is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6MjzylCb4dw"
      },
      "outputs": [],
      "source": [
        "split_ds = split_ds.filter(lambda e: e['sentence_word2vec'] is not None and e['compressed_word2vec'] is not None)\n",
        "print(split_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLO7aYXHpSOQ"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(split_ds['train'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GSPB0hJ6pSOQ"
      },
      "source": [
        "Huggingface datasets can return the columns as different types. Our embeddings are numpy arrays and it is a lot faster to return them as their type. The next cell will cast our dataset to return columns as numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqZenkH-pSOQ"
      },
      "outputs": [],
      "source": [
        "split_ds.reset_format()\n",
        "np_ds = split_ds.with_format('np', columns=['sentence_word2vec', 'compressed_word2vec'], dtype=float)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xGxYoIO8pSOQ"
      },
      "source": [
        "Here you can see that the new dataset returned a single numpy array containing all sentence embeddings in our dataset. This is a lot more efficient than returning a list of arrays (which is the default behaviour). Below we check the type and the dimensionality.\n",
        "\n",
        "We will be using `text` subset from our dataset to not use too much RAM. But if your system has enough memory, try to use `train` subset here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UYbFBUppSOQ"
      },
      "outputs": [],
      "source": [
        "sent_word2vec = np_ds['test']['sentence_word2vec']\n",
        "compr_word2vec = np_ds['test']['compressed_word2vec']\n",
        "print(type(sent_word2vec))\n",
        "print(sent_word2vec.shape)\n",
        "print(type(compr_word2vec))\n",
        "print(compr_word2vec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Q_MTJOfHpSOQ"
      },
      "source": [
        "## 5. Retrieving Sentences\n",
        "In this section we will try use the vector representations in our dataset to retrieve only the relevant ones based on some user query. You can think of it as a search retrieval task (based on what we discussed in the relevant lecture) where a user provides a query (that is the compressed sentence) and the system returns the sentences that are more similar to the query.\n",
        "\n",
        "In the information retrieval lecture, we discussed how to solve this retrieval problem by using bag-of-words as a representation basis. Here, we will use word embeddings instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uym4VBkPpSOQ"
      },
      "source": [
        "<a name='e8'></a>\n",
        "### Exercise 8 (points 5)\n",
        "\n",
        "First step to a retrieval task is to embed the query (aka find a representation). We will do it the same way as we did for the sentences in our dataset. Complete the following function to return the embedding of the provided text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOgQCgC6pSOQ"
      },
      "outputs": [],
      "source": [
        "def embed_query_word2vec(query):\n",
        "    \"\"\"\n",
        "    Embeds the provided query using trained Word2Vec model.\n",
        "    The query is first cleaned and tokenized (with the `clean()` and `tokenize() functions.\n",
        "    Args:\n",
        "        query: a str with the query\n",
        "\n",
        "    Returns: a numpy array with the embedded qury\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cRX5rrKPn91"
      },
      "source": [
        "Next we try the condensed representatin based on a simple query. Feel free to try different queries with different words. What happens if we have OOV words in a query?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La5aOnDfpSOQ"
      },
      "outputs": [],
      "source": [
        "query = \"volcano erupted\"\n",
        "print(query)\n",
        "\n",
        "query_word2vec = embed_query_word2vec(query)\n",
        "print(query_word2vec.shape)\n",
        "print(query_word2vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "K4xqPMiZpSOR"
      },
      "source": [
        "<a name='e9'></a>\n",
        "### Exercise 9 (points 5)\n",
        "\n",
        "The next step in our retrieval system, would be to calculate the proximity of a query to our retrieval corpus (in our case that is all the sentences).\n",
        "\n",
        "Complete the following function to calculate the cosine similarity between a vector (first parameter `vector`, that will usually be the query vector) and all other vectors (second parameter `other_vectors`, that will be the sentence embeddings in our case). Note that the `other_vectors` parameter is a single numpy array of size `N x D`, where $N$ is the number of vectors and $D$ is the dimension of each vector.\n",
        "\n",
        "For maximum efficiency (we will need it) do not use loops. Try to write the implementation with numpy functions. Hint: matrix multiplication can be seen as calculating the dot product between rows and columns of the multiplied matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgaV2b3fpSOR"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity_1_to_n(vector, other_vectors):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity between a single vector and other vectors.\n",
        "    Args:\n",
        "        vector: a numpy array representing a vector of D dimensions\n",
        "        other_vectors: a 2D numpy array representing other vectors (of the size NxD, where N is the number of vectors and D is their dimension)\n",
        "\n",
        "    Returns: a 1D numpy array of size N containing the cosine similarity between the vector and all the other vectors\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "irNqAJZLpSOR"
      },
      "source": [
        "We will use the function to calculate the similarity of all sentences in the dataset to our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD9mVAhYpSOR"
      },
      "outputs": [],
      "source": [
        "query_similarity = cosine_similarity_1_to_n(query_word2vec, sent_word2vec)\n",
        "print(query_similarity.shape)\n",
        "print(query_similarity[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_l3erVTNpSOR"
      },
      "source": [
        "The following cell will select the most similar sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wWa4AWHpSOR"
      },
      "outputs": [],
      "source": [
        "most_similar = int(np.argmax(query_similarity))\n",
        "print(most_similar)\n",
        "print(query_similarity[most_similar])\n",
        "print(split_ds['test'][most_similar]['set'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fpdJHFD1pSOR"
      },
      "source": [
        "The following function will return the indices of the top-k elements in the array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFUNI9elpSOS"
      },
      "outputs": [],
      "source": [
        "def top_k_indices(array, k, sorted=True):\n",
        "    \"\"\"\n",
        "    Returns top-k indices from the 1D array. If `sorted` is `True` the returned indices are sorted in the descending order\n",
        "    Args:\n",
        "        array: a 1D numpy array\n",
        "        k: a number of top indices to return\n",
        "        sorted: if True, the returned indices are sorted in descending order\n",
        "\n",
        "    Returns: a 1D array containing top-k indices\n",
        "\n",
        "    \"\"\"\n",
        "    top_k = np.argpartition(array, -k)[-k:]\n",
        "    if sorted:\n",
        "      selected = array[top_k]\n",
        "      sorted_selected = (-selected).argsort()\n",
        "      top_k = top_k[sorted_selected]\n",
        "    return top_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY8vvJdepSOS"
      },
      "outputs": [],
      "source": [
        "top_indices = top_k_indices(query_similarity, k=10).tolist()\n",
        "for idx in top_indices:\n",
        "    print(split_ds['test'][idx]['set'][0])\n",
        "    print(f'similarity: {query_similarity[idx]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "BuRI2JzQpSOS"
      },
      "source": [
        "<a name='e10'></a>\n",
        "### Exercise 10 (points 5)\n",
        "\n",
        "Experiment with different queries (taking into account the nature of the dataset and your insights from the analysis so far).\n",
        "\n",
        "Does the search perform well? When does it fail? Discuss several examples that are we get an expected but also unexpected results (find at least 5 from each category). Try to provide reasons for the good/bad result in each case (e.g. is there some error in the data, is there some linguistic phenomenon that we don't capture, is something wrong with our modeling with average embeddings, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEQKHAvcpSOS"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiIGTfq6eFI5"
      },
      "source": [
        "// your comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2jouFmeHOg"
      },
      "source": [
        "## 5. Evaluating Retrieval\n",
        "\n",
        "In this last section we will try to evaluate how good our sentence retrieval system is. To keep the computational resources manageable, we will use the test set for that as its size is more manageable.\n",
        "\n",
        "Recall from the lecture in IR that there are several metrics to evaluate retrieval performance by taking into account the relevance of the retrieved results to the query. We will use Recall@K here (for more metrics and more details refer to the lecture slides and the textbooks).\n",
        "\n",
        "RRecall@K is a metric used to measure the effectiveness of a search system in retrieving relevant documents within the top $K$ retrieved documents. It calculates the proportion of relevant documents retrieved within the top-$K$ results, compared to the total number of relevant documents in the collection.\n",
        "\n",
        "$\n",
        "\\text{Recall@K} = \\frac{\\text{Number of relevant documents retrieved in the top }-K}{\\text{Total number of relevant documents}}\n",
        "$\n",
        "\n",
        "In our case, we have a sentence, and it's compressed version. To test our system, we will treat compressed sentences as the queries. Each query will have only a single relevant sentence - the corresponding uncompressed sentence.\n",
        "\n",
        "Therefore, for the calculation of Recall@K we will take into account whether the correct retrieved result is contained within the first $K$ retrieved results. For example, if for a query (i.e. a compressed sentence) we retrieve 10 results and within these we see the relevant one (i.e. the full sentence), then Recall@10 = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUKPtG-uem9f"
      },
      "source": [
        "<a name='e11'></a>\n",
        "### Exercise 11 (points 5)\n",
        "\n",
        "In this exercise you will revisit your implementation of the cosine siliarity. Generalize it so that it can accept two arrays containing two sets of vectors (first one containing $M$ vectors and the second one $N$ vectors). Compute the cosine similarity between each pair of vectors coming from the two sets. The result should be an array of size $M x N$.\n",
        "\n",
        "Once again, try to write an efficient code. This means no loops. Remember the relation between matrix multiplication and dot product. (Depending on your implementation of the previous function calculating cosine similarity, this one can be almost the same)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKiHDrN9eld7"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity_m_to_n(vectors, other_vectors):\n",
        "    \"\"\"\n",
        "    Calculates the cosine similarity between a multiple vectors and other vectors.\n",
        "    Args:\n",
        "        vectors: a numpy array representing M number of vectors of D dimensions (of the size MxD)\n",
        "        other_vectors: a 2D numpy array representing other vectors (of the size NxD, where N is the number of vectors and D is their dimension)\n",
        "\n",
        "    Returns: a numpy array of cosine similarity between all the vectors and all the other vectors\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ-Yhl1Sgoka"
      },
      "source": [
        "The following function will use your implementation to calculate Recall@K based on the similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0sLS3uRCfdh"
      },
      "outputs": [],
      "source": [
        "def calculate_recall(queries, sentences, k, batch_size=1000):\n",
        "  \"\"\"\n",
        "  Calculates recall@k given the embeddings of the queries and sentences.\n",
        "  Assumes that only a single sentence with the same index as query is relevant.\n",
        "  Batching is implemented to avoid high memory usage.\n",
        "  Args:\n",
        "      queries: a numpy array with the embeddings of N queries\n",
        "      sentences: a numpy array with the embeddings of N sentences available for retrieval\n",
        "      k: number of top results to search for the relevant sentence\n",
        "      batch_size: number of queries to process at a time\n",
        "\n",
        "  Returns: calculated recall@k\n",
        "\n",
        "  \"\"\"\n",
        "  n_queries = queries.shape[0]\n",
        "  if batch_size is None:\n",
        "    batch_size = n_queries\n",
        "\n",
        "  n_batches = math.ceil(n_queries / batch_size)\n",
        "  last_batch_size = n_queries % batch_size if n_queries != batch_size else batch_size\n",
        "\n",
        "  correct = np.zeros(n_queries).astype(bool)\n",
        "  with tqdm.tqdm(total=n_queries) as pbar:\n",
        "    for b in range(n_batches):\n",
        "      effective_batch_size = last_batch_size if b == (n_batches - 1) else batch_size\n",
        "      batch_start_index = b * batch_size\n",
        "\n",
        "      queries_batch = queries[batch_start_index:batch_start_index + effective_batch_size]\n",
        "      batch_similarity = cosine_similarity_m_to_n(queries_batch, sentences)\n",
        "\n",
        "      for i in range(effective_batch_size):\n",
        "        query_index = i + batch_start_index\n",
        "        query_similarity = batch_similarity[i]\n",
        "        top_k = top_k_indices(query_similarity, k=k, sorted=False)\n",
        "\n",
        "        if query_index in top_k:\n",
        "            correct[query_index] = True\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "  n_correct = np.sum(correct)\n",
        "  n_total = correct.shape[0]\n",
        "  recall = n_correct / n_total\n",
        "  return recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QgAgMiDgw8m"
      },
      "source": [
        "You can use it like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxpex7ZxHM7c"
      },
      "outputs": [],
      "source": [
        "recall_at_1 = calculate_recall(compr_word2vec, sent_word2vec, k=1, batch_size=1000)\n",
        "print(f'\\n{recall_at_1 * 100:.2f}%' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY5ZVq5ogzMI"
      },
      "source": [
        "<a name='e12'></a>\n",
        "### Exercise 12 (points 5+10)\n",
        "\n",
        "(5 points) Calculate recall for different values of $K$. Comment on how recall changes based on the value of $K$. Are the results expected or surprising? Comment again on different examples (like in Exercise 10) but now take into account the results of recall at different levels of $K$.\n",
        "\n",
        "Open question (10 points): Try to improve the scores by e.g. tuning the word2vec parameters or using pre-trained embeddings. Discuss the results you achieve, even if you didn't manage to improve the scores.\n",
        "\n",
        "Note: Pre-trained embeddings might be difficult to load and use (memory requirements) so you might have to work locally there. You don't need to submit (really, don't!) the pre-trained word embeddings you used but make sure to include a link in your delivered notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA9bEm5ehMq8"
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### YOUR CODE ENDS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etkj0Cc-hPQs"
      },
      "source": [
        "// your comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gwVkSMlhQIY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}